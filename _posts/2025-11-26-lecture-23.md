---
layout: distill
title: Lecture 23
description: Supervised Fine-tuning of LLMs
date: 2025-11-26

lecturers:
  - name: Ben Lengerich
    url: "https://adaptinfer.org"

authors:
  - name: Eva Song # author's full name
    url: NA # optional URL to the author's homepage
  - name: Author 2
    url: "#"
  - name: Author 3
    url: "#"

# editors:
#   - name: Editor 1 # editor's full name
#     url: "#" # optional URL to the editor's homepage

abstract: >
  This lecture introduces techniques for post-training adaptation of Large Language Models, including the supervised fine-tuning (SFT) of LLMs, reinforcement learning from human feedback (RLHF), Reinforcement Learning with Verifiable Rewards (RLVR), and some techiniques in parameter-efficient fine-tuning (PEFT).
---
## 1. The Transition from Pre-training to Fine-tuning

The initial training of Generative Pre-trained Transformer (GPT) models is an unsupervised process designed to model the probability distribution of language, but it does not inherently equip the model for specific, goal-oriented tasks.

### 1.1 The Foundation: Unsupervised Pre-training Objective

Recall in Directed PGM for autoregressive model, the core training objective for GPT models is Maximum Likelihood Estimation (MLE). This probabilistic goal aims to maximize the log-likelihood of observed text sequences by learning to predict the next token given the preceding ones. 

<figure>
  <img src="{{ '/assets/img/notes/lecture-23/obj_of_GPT_MLE.png' | relative_url }}" style="width:80%; display:block; margin:auto;" />
  <figcaption><strong>Figure 1.</strong> The objective of Directed PGM is MLE</figcaption>
</figure>

The objective is formally expressed as:

$$
\max_{\theta} \sum_{i}\sum_{t} logP_{\theta}(X_{i,t}|X_{i,< t})
$$

This process effectively trains the model on the underlying structure and patterns of language (`P(X)`) found in the training data.

### 1.2 Inherent Limitations of Pre-training

While powerful for learning language, the MLE pre-training objective has significant limitations regarding the model's practical usefulness. The source material explicitly identifies several missing components:

- **No task goals:** The model is not trained to accomplish a specific objective.
- **No explicit reward:** There is no mechanism to reinforce desirable outputs over undesirable ones.
- **No utility:** The concept of a "useful" or "helpful" response is not part of the training function.
- **No semantics:** The model learns statistical relationships, not a deep understanding of meaning.

These limitations necessitate a subsequent phase of fine-tuning to transform the base model into a useful and aligned assistant. The source underscores that in this entire process, "Dataset selection drives everything."

## 2. Core Fine-Tuning Methodologies

To imbue pre-trained LLMs with utility, supervised and reinforcement learning techniques are applied to align model behavior with human intent and task requirements.

### 2.1 Supervised Fine-Tuning (SFT)

Supervised Fine-Tuning is a direct method to teach a language model how to respond appropriately to various types of prompts. The methodology is characterized as a form of **"Behavior cloning,"** where the model learns to imitate high-quality, human-generated responses from a curated dataset.

The impact of SFT is substantial. The development of **InstructGPT** demonstrated that a 1.3 billion parameter model, after undergoing SFT, could outperform a much larger 175 billion parameter base GPT model, highlighting the efficiency and power of this alignment technique.

### 2.2 Reinforcement Learning from Feedback

Reinforcement Learning introduces an explicit reward signal, allowing the model to be optimized directly for desired outcomes.

#### Reinforcement Learning with Human Feedback (RLHF)

RLHF uses human preferences to train a reward model, which is then used to fine-tune the LLM. The success of this approach is critically dependent on the quality of the input data, with the source noting that **"High-quality data is critical."** An open question in the field, attributed to John Schulman (2023), is whether human feedback effectively reduces model hallucinations.

#### Reinforcement Learning with Verifiable Rewards (RLVR)

RLVR is presented as a superior alternative to RLHF in domains where outputs can be objectively verified. Instead of relying on subjective human feedback, RLVR uses **"verifiable truth"** as its reward signal. This is particularly effective for tasks with clear success criteria, including:

- **Code generation:** Does the generated code run correctly?
- **Math questions:** Is the provided solution mathematically correct?
- **Formatting-specifics:** Does the output adhere to all specified formatting requirements?

## 3. Parameter-Efficient Adaptation and Personalization

Adapting a massive LLM for every individual user or task is computationally prohibitive. Parameter-efficient methods have been developed to enable efficient and scalable personalization.

### 3.1 The Challenge of Personalization

The core challenge is that "Every user has their own preferences, history, and contexts." To be truly effective, models must be able to adapt to these unique factors efficiently.

### 3.2 Low-Rank Adaptation (LoRA)

LoRA is a prominent parameter-efficient fine-tuning technique built on a key insight about model adaptation. The central hypothesis of LoRA is:

- The change in weights during model adaptation has a low “intrinsic rank.”

This means that instead of fine-tuning all the model's billions of parameters, LoRA introduces a small number of trainable parameters that represent this low-rank change, drastically reducing the computational cost of adaptation.

### 3.3 Retrieval-Augmented Generation (RAG)

RAG is another powerful technique for personalization and adaptation. Its core principle is that **"Resource access enables personalization."** By allowing the model to retrieve information from an external knowledge base (which can include user-specific documents or data), RAG enhances the model's responses with relevant, up-to-date, and context-aware information without requiring modifications to the model's weights. The lecture materials also reference a specific advanced implementation, **RAG of Interpretable Models (RAG-IM)**.
