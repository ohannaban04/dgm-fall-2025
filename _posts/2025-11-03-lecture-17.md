---
layout: distill
title: "Lecture 17"
lecturers:
  - Ben Lengerich
authors:
  - Erica Henderson
editors:
  -
date: 2025-11-03
permalink: /notes/lecture-17/
bibliography: /assets/bibliography/2025-11-03-lecture-17.bib
---

## Overview

Generative Adversarial Networks (GANs) were introduced by Goodfellow et al. (2014), and is a generative modeling framework between a **generator** that produces synthetic samples and a **discriminator** that tries to distinguish them from real data. Unlike autoencoders or autoregressive models, GANs can generate an entire sample with less steps.

---

## Motivation

Traditional generative models (Gussian Mixtures, VAEs, and autoencoders) often struggle to uncover the complexity of high-dimensional data like that of images. GANs are flexible and capable of learning **implicit data distributions**. This makes them useful for image synthesis, style transfer, and text to image applications like those developed by OpenAI (DALL-E 2) and Google (Imagen).

---

## Architecture and Training

### Generator ($G_\theta$)

- Maps a noise vector $z \sim p(z)$ (often Normal(0, I)) to a data-space sample $x = G_\theta(z)$.
- Goal: produce samples indistinguishable from real data $x \sim p_{data}$.
- Fool the discriminator (increase $D(G(z))$)
- Trained via gradient descent to maximize $D(G(z))$.

### Discriminator ($D_\phi$)

- Binary classifier outputting $D_\phi(x)\in[0,1]$, - Trained via gradient ascent to maximize $\log D(x)$ for real data
  and minimize $\log (1 - D(G(z)))$ for fake data.
- Gradient update: Gradient _ascent_

### Minimax Objective

$$
\min_{G}\max_{D}\;
\mathbb{E}_{x\sim p_{data}}[\log D(x)] +
\mathbb{E}_{z\sim p(z)}[\log (1-D(G(z)))].
$$

The generator minimizes this value by making its outputs hard to distinguish,
while the discriminator maximizes it by improving classification.

---

## Training Characteristics

Training alternates between both networks. Convergence ideally occurs at **Nash Equilibrium**, where the generator’s distribution equals the true data distribution and the discriminator outputs 0.5 for all inputs.

In practice training is often unstable:

- Oscillatory losses between G and D
- **Mode collapse** (one prototype sample repeated)
- **Vanishing gradients** if D dominates this can lead to non-saturating loss being recommended
- **Hyperparameter sensitivity** (learning rate, architecture, batch size)

---

## Interpretations and Variants

A pure equilibrium may not exist, which explains observed oscillations and non-convergence.

### Deep Convolutional GAN (DC-GAN)

Introduces convolutional architectures to stabilize training and capture spatial features.

---

## GANs vs VAEs and Variational-EM View

| Aspect              | Variational Autoencoder (VAE) | Generative Adversarial Network (GAN)             |
| ------------------- | ----------------------------- | ------------------------------------------------ |
| **Objective**       | Single ELBO maximization      | Two opposing objectives ($\min_G$, $\max_D$)     |
| **Regularization**  | KL term via prior $p(z)$      | Implicit regularization via adversarial feedback |
| **Inference Model** | $q_\phi(z \mid x)$            | $p_\theta(x \mid y)$, $q_\phi(y \mid x)$         |
| **Generation**      | Explicit probability model    | Implicit distribution (no likelihood)            |

GANs can be expressed in a variational-EM-like framework:

- **E-step:** update discriminator to approximate $q_\phi(y \mid x)$
- **M-step:** update generator to improve $p_\theta(x \mid y)$

---

## Common Problems

| Problem                    | Explanation                         | Typical Fixes                                        |
| -------------------------- | ----------------------------------- | ---------------------------------------------------- |
| **Mode Collapse**          | Generator outputs few modes of data | Mini-batch discrimination, WGAN, feature matching    |
| **Vanishing Gradient**     | Discriminator too strong            | Non-saturating loss (maximize $\log D(G(z))$)        |
| **Training Oscillation**   | No stable equilibrium               | Gradient penalty, slow updates, learning-rate tuning |
| **Over-fit Discriminator** | Memorizes training data             | Dropout, label smoothing                             |

Empirically, GANs generalize only when the discriminator capacity and training data are balanced.  
Otherwise, Jensen–Shannon and Wasserstein divergence analyses can be misleading in finite settings.

---

### References

- Goodfellow et al. (2014) _Generative Adversarial Nets_ (NIPS).
- Arora & Hardt (2017) _Generative Adversarial Networks: Some Open Questions_ (OffConvex Blog).
- Radford et al. (2015) _Unsupervised Representation Learning with Deep Convolutional GANs_.
- Hu et al. (2017) _Unifying Deep Generative Models_.
